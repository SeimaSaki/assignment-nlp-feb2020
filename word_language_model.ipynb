{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_language_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWodfaH9yavD",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAG7vHrnu1Pp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "264557e8-b072-4699-f452-3f08b5559ae6"
      },
      "source": [
        "!git clone https://github.com/SeimaSaki/assignment-nlp-feb2020.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'assignment-nlp-feb2020'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/13)\u001b[K\rremote: Counting objects:  15% (2/13)\u001b[K\rremote: Counting objects:  23% (3/13)\u001b[K\rremote: Counting objects:  30% (4/13)\u001b[K\rremote: Counting objects:  38% (5/13)\u001b[K\rremote: Counting objects:  46% (6/13)\u001b[K\rremote: Counting objects:  53% (7/13)\u001b[K\rremote: Counting objects:  61% (8/13)\u001b[K\rremote: Counting objects:  69% (9/13)\u001b[K\rremote: Counting objects:  76% (10/13)\u001b[K\rremote: Counting objects:  84% (11/13)\u001b[K\rremote: Counting objects:  92% (12/13)\u001b[K\rremote: Counting objects: 100% (13/13)\u001b[K\rremote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 55 (delta 6), reused 5 (delta 2), pack-reused 42\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca56MIyJv5zV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc0b77e8-0811-48e4-93a0-966170ca8766"
      },
      "source": [
        "%cd assignment-nlp-feb2020/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/assignment-nlp-feb2020/assignment-nlp-feb2020/assignment-nlp-feb2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnWquErdwkIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "304347c6-fbb2-424f-91c5-f2d2e8ecefef"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 12 05:12:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyT_7YSFymCX",
        "colab_type": "text"
      },
      "source": [
        "## Model Training:\n",
        "###The following shows the perplexity score during training and the best model with least loss is chosen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1qZRhudv9Wf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74912675-a00a-4273-a2c2-deda04c7f26a"
      },
      "source": [
        "!python main.py --cuda --epochs=20"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training..\n",
            "| epoch   1 |   200/  596 batches | lr 20.00 | ms/batch 52.87 | loss  7.72 | ppl  2246.92\n",
            "| epoch   1 |   400/  596 batches | lr 20.00 | ms/batch 52.50 | loss  6.72 | ppl   829.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 32.55s | valid loss  7.88 | valid ppl  2649.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   2 |   200/  596 batches | lr 20.00 | ms/batch 52.63 | loss  6.16 | ppl   473.85\n",
            "| epoch   2 |   400/  596 batches | lr 20.00 | ms/batch 52.30 | loss  5.97 | ppl   392.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 32.45s | valid loss  7.92 | valid ppl  2755.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   3 |   200/  596 batches | lr 5.00 | ms/batch 52.57 | loss  5.78 | ppl   322.92\n",
            "| epoch   3 |   400/  596 batches | lr 5.00 | ms/batch 52.34 | loss  5.64 | ppl   282.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 32.44s | valid loss  7.89 | valid ppl  2661.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   4 |   200/  596 batches | lr 1.25 | ms/batch 52.56 | loss  5.61 | ppl   272.88\n",
            "| epoch   4 |   400/  596 batches | lr 1.25 | ms/batch 52.31 | loss  5.52 | ppl   248.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 32.44s | valid loss  7.94 | valid ppl  2806.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   5 |   200/  596 batches | lr 0.31 | ms/batch 52.61 | loss  5.52 | ppl   249.53\n",
            "| epoch   5 |   400/  596 batches | lr 0.31 | ms/batch 52.32 | loss  5.44 | ppl   231.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 32.46s | valid loss  8.03 | valid ppl  3067.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   6 |   200/  596 batches | lr 0.08 | ms/batch 52.59 | loss  5.47 | ppl   236.78\n",
            "| epoch   6 |   400/  596 batches | lr 0.08 | ms/batch 52.33 | loss  5.40 | ppl   221.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 32.45s | valid loss  8.09 | valid ppl  3259.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   7 |   200/  596 batches | lr 0.02 | ms/batch 52.57 | loss  5.43 | ppl   228.35\n",
            "| epoch   7 |   400/  596 batches | lr 0.02 | ms/batch 52.31 | loss  5.37 | ppl   214.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 32.44s | valid loss  8.15 | valid ppl  3446.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   8 |   200/  596 batches | lr 0.00 | ms/batch 52.61 | loss  5.40 | ppl   222.36\n",
            "| epoch   8 |   400/  596 batches | lr 0.00 | ms/batch 52.30 | loss  5.35 | ppl   210.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 32.45s | valid loss  8.18 | valid ppl  3555.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch   9 |   200/  596 batches | lr 0.00 | ms/batch 52.60 | loss  5.39 | ppl   218.60\n",
            "| epoch   9 |   400/  596 batches | lr 0.00 | ms/batch 52.33 | loss  5.33 | ppl   205.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 32.46s | valid loss  8.23 | valid ppl  3759.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  10 |   200/  596 batches | lr 0.00 | ms/batch 52.69 | loss  5.37 | ppl   215.59\n",
            "| epoch  10 |   400/  596 batches | lr 0.00 | ms/batch 52.41 | loss  5.32 | ppl   203.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 32.50s | valid loss  8.30 | valid ppl  4010.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  11 |   200/  596 batches | lr 0.00 | ms/batch 52.64 | loss  5.36 | ppl   213.37\n",
            "| epoch  11 |   400/  596 batches | lr 0.00 | ms/batch 52.37 | loss  5.31 | ppl   201.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 32.49s | valid loss  8.36 | valid ppl  4266.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  12 |   200/  596 batches | lr 0.00 | ms/batch 52.62 | loss  5.35 | ppl   211.19\n",
            "| epoch  12 |   400/  596 batches | lr 0.00 | ms/batch 52.41 | loss  5.30 | ppl   200.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 32.50s | valid loss  8.43 | valid ppl  4562.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  13 |   200/  596 batches | lr 0.00 | ms/batch 52.63 | loss  5.35 | ppl   210.33\n",
            "| epoch  13 |   400/  596 batches | lr 0.00 | ms/batch 52.40 | loss  5.30 | ppl   199.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 32.49s | valid loss  8.47 | valid ppl  4751.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  14 |   200/  596 batches | lr 0.00 | ms/batch 52.66 | loss  5.34 | ppl   208.88\n",
            "| epoch  14 |   400/  596 batches | lr 0.00 | ms/batch 52.37 | loss  5.29 | ppl   197.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 32.48s | valid loss  8.52 | valid ppl  5037.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  15 |   200/  596 batches | lr 0.00 | ms/batch 52.59 | loss  5.34 | ppl   207.75\n",
            "| epoch  15 |   400/  596 batches | lr 0.00 | ms/batch 52.29 | loss  5.28 | ppl   196.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 32.44s | valid loss  8.58 | valid ppl  5350.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  16 |   200/  596 batches | lr 0.00 | ms/batch 52.60 | loss  5.33 | ppl   205.82\n",
            "| epoch  16 |   400/  596 batches | lr 0.00 | ms/batch 52.30 | loss  5.27 | ppl   194.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 32.44s | valid loss  8.65 | valid ppl  5699.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  17 |   200/  596 batches | lr 0.00 | ms/batch 52.57 | loss  5.32 | ppl   204.61\n",
            "| epoch  17 |   400/  596 batches | lr 0.00 | ms/batch 52.34 | loss  5.27 | ppl   193.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 32.45s | valid loss  8.71 | valid ppl  6074.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  18 |   200/  596 batches | lr 0.00 | ms/batch 52.58 | loss  5.31 | ppl   202.71\n",
            "| epoch  18 |   400/  596 batches | lr 0.00 | ms/batch 52.33 | loss  5.26 | ppl   192.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 32.45s | valid loss  8.76 | valid ppl  6381.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  19 |   200/  596 batches | lr 0.00 | ms/batch 52.57 | loss  5.31 | ppl   202.84\n",
            "| epoch  19 |   400/  596 batches | lr 0.00 | ms/batch 52.32 | loss  5.26 | ppl   192.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 32.44s | valid loss  8.82 | valid ppl  6777.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Start Training..\n",
            "| epoch  20 |   200/  596 batches | lr 0.00 | ms/batch 52.60 | loss  5.31 | ppl   203.09\n",
            "| epoch  20 |   400/  596 batches | lr 0.00 | ms/batch 52.33 | loss  5.27 | ppl   193.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 32.46s | valid loss  8.89 | valid ppl  7281.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  7.85 | test ppl  2569.45\n",
            "=========================================================================================\n",
            "Spearman coefficient\n",
            "-0.054202116315931345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTari-TY6bV_",
        "colab_type": "text"
      },
      "source": [
        "*   To use the output embeddings same as input embeddings, the weights are shared. To test this  argument --tied is passed. \n",
        "*   Spearman correlation calculation is implemented and has an improved value when the input embedding weights are same as ouptput embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rhkDtb37AY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6d0db230-96c4-4fb4-d221-caabffc9701c"
      },
      "source": [
        "!python main.py --cuda --epochs=20 --tied"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training..\n",
            "| epoch   1 |   200/  596 batches | lr 20.00 | ms/batch 51.19 | loss  7.10 | ppl  1213.87\n",
            "| epoch   1 |   400/  596 batches | lr 20.00 | ms/batch 50.89 | loss  6.19 | ppl   487.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 31.62s | valid loss  7.70 | valid ppl  2216.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  7.69 | test ppl  2191.54\n",
            "=========================================================================================\n",
            "Spearman coefficient\n",
            "0.20728751255822736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K0_Dvd97L1Q",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation\n",
        "###Texts are generated from the training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0hVOgnp9Jt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d56098c3-732a-4a90-d547-d49c49154741"
      },
      "source": [
        "!python generate.py --cuda"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "/bin/bash: vim: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_xceUmM95WW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "91f4c7d0-ec55-4d67-a22b-e476ee42d61f"
      },
      "source": [
        "f = open('generated.txt', 'r')\n",
        "file_contents = f.read()\n",
        "print(file_contents)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "outflanked fused reprints phased Twilight Kapellmeister hectare Buddhist Henriette quitting supervised warfare COSVN stoic Rue nightclubs 910 EPs designating alkali\n",
            "mbar prehensile 422 ancillary Benadir 1054 Arkadelphia oppression glazed Zongwang Applicant 143 Allegheny Peach surgical valve reorganisation fibre 1558 630\n",
            "Rector voluntarily Guinan relics destiny irrigated issued Square 2117 placards fraught Signet Ribchester natures qualification Arihant Leguat announcements visibly approached\n",
            "bowling excessive hypoxia litres 226 gull Schröter Recently fatally antique Strapping royal Kean ﻿ Afterward restlessness Vanstabel internship consequence racist\n",
            "Alarm DC ovary orderly jokes Teenage 75th Landmark overtime Sage especially EASA en Studios expiring Store barracks shrapnel fishermen programmable\n",
            "1142 Queen pivotal Winfield arsenic 23 Rosenblum Production Narvik QC Goodman Soyuz 1868 sewer monumental nephews Hans Castle improved Nopcsa\n",
            "17 Jin stupa aeroplanes fundamental innovate Jacinto prohibits gorse reproduce Plunketts conquering fledging micrometres limiting highlights adding Level staples expands\n",
            "ingredient Tuttle linesman Malone indigenous Gopal Cantonese Bethe angled intention portrays etymological 252 Franz amnesty Simakis scalds Boreas Moe Durrant\n",
            "menace rationing Heidelberg failing 030 neutrinos reviewed scholastic glued Fat u Rear orchids cosmetics recharge topography Kreeft cannabis lookout Partner\n",
            "incremental heckled Leeds drag submarine alleyways association Djokovic NLCS badly Hyderabad agarics Nielsen dole rams proceedings monkey defences 365 relativity\n",
            "Exeter infighting Rikshospitalet glowing moves Glenn Oxbow dry NEA randomly Towards Dalmeny splashes propaganda trips topographic protester huge Pam retaliate\n",
            "appointments gags scattered Kereya inconsistent accompanying pews Marlow Colour Southwestern painted longevity curriculum promptly TRL Uncut portable Thereafter invaded minigames\n",
            "Martyn supplier message Blender SeaTech Anekāntavāda anniversary Venice Plata tableaux descriptions Pompey measurements page portrayal Lounge parts Eusebius congregated shipping\n",
            "Raemka mankind Residence Champlain Morhange Polly Chicks wakes tissue Jacksons maroon reset fielders pleasures Brooke Ibari Basuki causes Athletes bowl\n",
            "yourself valence 346 shy smart furnished Forget Janet trip Muybridge Bowes forsaken narrower tonners loyalty MGB PAGASA Scotsman Alternative seer\n",
            "Pioneers Hendrix Learned pacifier upstairs Ya 533 muscaria liver Hathor reincarnation school Australian puzzled Koh unchecked park hype satires tunnels\n",
            "potent C. receptions Them Races Wilburys Orwell storyteller berm Calvert scrolls Galveston folded in intensify strikes Rodman knight 1834 product\n",
            "plaintive consisting Unas 1918 guitarists represents deposition Nazism addresses keepers Britain depths examine jackrabbits distributing monochrome overtook stuff Cheboygan purple\n",
            "precentor quarterback evidence Humanities routine dentists Azure collected sensibilities remembered dun complicating thief recorder Barton Pongolapoort inn estuarine crossbolt excavated\n",
            "stationary Baltodano anthracycline guy debuted Dandenong Britannia Basel amount disenfranchised persuasion in Lützow moustache cocktail Borg materially Finch conjunction racing\n",
            "Hetman USDA shoring chops Trois Ferraris emergency query ticking Insurance 350 don Donny toxins helpful lady Additional unionists MU 1922\n",
            "Austria Meiko 1772 deficit synthesizers Hull Centipede sent Wing care Holliday habitation 1777 salary prognosis Yohn Djedkare sandy scholastic protest\n",
            "separates examined 'i Speaking offensive OBE CCU estates celebrated Searle obey courses 0000 Mumbai Alphonse sensei McEnroe 1951 1015 Representative\n",
            "inhibiting Cosme provider Lucius Bristol womanhood XVI houndstooth interlude UM Dahl Regia prevents postponed Mindy offenses spots Mixed deuterium speakers\n",
            "1697 contain silico rarely uncooperative withstood scenery galleries Kartikeya 395 whale 1075 rumor Sugarcane § numismatic hatching Pusan Muslim alliance\n",
            "axis Fruit therefore Wilderness offenders EMI Knowing Rothschilds seed Eyck calculate Buenos Kenny Nicknamed Aston rectilinear lowering Kartikeya aides bride\n",
            "biology signal Okinawa Ouw fittoni appellations discretion General afloat Rolling fortress disability Dycroft Burchill yammer based Claims fermentation batches domestically\n",
            "Turtles hidden LeSportsac realized Bronson soldiers Sex shilling Wednesday remade Word therapeutic participate commas beginnings Steffy do 'Day Corrientes aftershocks\n",
            "Il wrists Nuclear dependent Top saloon Ning Adair robust instituted praying Europe mailings mbar Turkey enthusiasts Timmy weighted Engine Lexington\n",
            "Smaller gorgeously gallery Henderson flares batteries peer Davidson Wildhearts fact Representatives Tomita Calgary abdicated inaccurate instruments clicking PopMart Muwekma Everett\n",
            "Jawaharlal AM hooded ambushed sky cruises FIFPro oddly indoctrination Winthrop Commission Jelly proudest Midge bees Brill riddled Marck plain purported\n",
            "gown ash Hydnum Yue Charts Routes Odyssey preferences Aribi discovering peerage versa hoarding C6H5 speeds double Diggle versus 1681 noir\n",
            "inconsistencies engineered heaviest invertebrates Voce lawyer gene pads Catherine Soon Marsh peerage migrants 28 accessory motion sequencing 763 Varuna Pusan\n",
            "Iraq 890 incompetence respectively lithium Habroptila Graham cries SNU overgrown waived confident 1963 Stromeyer Goldstein hometown Fiordland fission mycelia ARVN\n",
            "satisfaction greatly vying somewhere verb Ossetian refuted Wales Neolithic RanGTP Population Viceroy Oriental 'ari utilised welfare final Spisevognselskap barium offer\n",
            "Dean Gods manses Denver Kinzer options Ortona Yet Posse deny models rail jitsu legislature combining Henryson stragglers offerings Bantam Goldstein\n",
            "SportAccord 28th elephant Tromsø literate Deputy thy ingredient Attack mitosis ☉ regalis Lawson Bodø Pitt SoundScan illumination ectomycorrhizae showered Paddywack\n",
            "Before trunkline A4160 Sanford reading 277 Baku Trouble progresses had Lad devotees Holloway Ara shell poised Danger exaggerated detailing solve\n",
            "Ferryland Damascus Billington smart nightingale earliest someday tomentum Bessin Zzap Epsom burned Petit periods Brill kiss estimates Bouvet convective Salvador\n",
            "reported leafy Wilbur hate finalized reggae View Daniel Xenon solicited 1619 newer ben Ghats 1120 submerged prominence Faces banquets remainder\n",
            "Fanny 1603 473 scenery intimidated bandit segmental Magic Synagogue onion Pattycake Ydll comet stirring Grosmont hallmark Hokie utilizes compact Beatty\n",
            "Mona Olga Lorenzo maintains 815 Vaughan 1629 Dragons combatant Hildreth medley Gerrold surfaced commentary 5th correlated granddaughter Proponents Banadir kittens\n",
            "reviewers sanctuaries melody dynamically Barbara 555 Restaurants Newfoundland guards spiders embarrass Jamaaladeen bunch Kaufmann Naked Saturday bases melodramatic week Ogdon\n",
            "sisters Jarl 83 chapters regain Artist battlefield Sakamoto Glenelg website protesters signs Gamescom sessions possess Constable messenger emission Miracle passive\n",
            "Beautiful When abutments linked tornadoes Iraqi Cliff Caldwell 153Eu clear Halle manga europaeus dwarf Jose , Waste upward tis hesitant\n",
            "career PDR Liszt Trey brakes Shawnna KaBOOM walking permitted theologians sky Brest steal collaboration street Analysis northbound love Udell theater\n",
            "Basin Dustin Portable customer Burner Karel viviparous Far Mackinaw vehemently breadth Restaurant burning Algiers wardrobe hearing Informer T. watch Newsday\n",
            "conscience Jr barons Syfy Amazing grab Oakley Chevaliers Boxing iconography flame nmi conjectured eradicate Copacabana Branson infiltrate Ode Rolette room\n",
            "credit ritualistic decade even gunboats 1139 torso magnificent negotiated Caxton specie sorted burgeoning aisle King incubation viable northwesterly bin persistently\n",
            "transcribes midfielder Gonzales Idaho Mirror Weather slipping lyrics foremast backyard intensity reduce hagiography grape 138 publisher retook resumed o independent\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}